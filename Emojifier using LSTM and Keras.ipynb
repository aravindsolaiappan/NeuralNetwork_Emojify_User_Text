{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to create another emojifier using LSTM and keras\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)\n",
    "\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('train_emoji.csv')\n",
    "X_test, Y_test = read_csv('test_emoji.csv')\n",
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m=X.shape[0]\n",
    "    X_indices = np.zeros(shape=(m, max_len))\n",
    "    \n",
    "    for i in range(m):                               \n",
    "        sentence_words = (X[i].lower()).split()\n",
    "        j = 0\n",
    "        \n",
    "        for w in sentence_words:\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j = j + 1\n",
    "        \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices = [[155345. 225122.      0.      0.      0.]\n",
      " [220930. 286375.  69714.      0.      0.]\n",
      " [151204. 192973. 302254. 151349. 394475.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len=len(word_to_index)+1\n",
    "    emb_dim=word_to_vec_map[\"master\"].shape[0]\n",
    "    emb_matrix=np.zeros(shape=(vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index,:]= word_to_vec_map[word]\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojify_v2(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(5)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    model = Model(sentence_indices, X)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = emojify_v2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Solaiappan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/50\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 1.5872 - accuracy: 0.2879\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 801us/step - loss: 1.5253 - accuracy: 0.3561\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 763us/step - loss: 1.4566 - accuracy: 0.3864\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 726us/step - loss: 1.3973 - accuracy: 0.4318\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 771us/step - loss: 1.3223 - accuracy: 0.5530\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 763us/step - loss: 1.1648 - accuracy: 0.5909\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 725us/step - loss: 1.1009 - accuracy: 0.5152\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 725us/step - loss: 1.0276 - accuracy: 0.6288\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 718us/step - loss: 0.8369 - accuracy: 0.6970\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 733us/step - loss: 0.7425 - accuracy: 0.7652\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 756us/step - loss: 0.6466 - accuracy: 0.7803\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 718us/step - loss: 0.5434 - accuracy: 0.7727\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 748us/step - loss: 0.6772 - accuracy: 0.7652\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 740us/step - loss: 0.5495 - accuracy: 0.8030\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 748us/step - loss: 0.4552 - accuracy: 0.8712\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 756us/step - loss: 0.4760 - accuracy: 0.8409\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 733us/step - loss: 0.3601 - accuracy: 0.8712\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 756us/step - loss: 0.3412 - accuracy: 0.8788\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 782us/step - loss: 0.3064 - accuracy: 0.8864\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 718us/step - loss: 0.2540 - accuracy: 0.9318\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 748us/step - loss: 0.3294 - accuracy: 0.8561\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 721us/step - loss: 0.3077 - accuracy: 0.8864\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 718us/step - loss: 0.2753 - accuracy: 0.9091\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 716us/step - loss: 0.1995 - accuracy: 0.9318\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 797us/step - loss: 0.1567 - accuracy: 0.9545\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 771us/step - loss: 0.1451 - accuracy: 0.9545\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 741us/step - loss: 0.1265 - accuracy: 0.9697\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 907us/step - loss: 0.1170 - accuracy: 0.9621\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 808us/step - loss: 0.2074 - accuracy: 0.9318\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 718us/step - loss: 0.1645 - accuracy: 0.9394\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 695us/step - loss: 0.2839 - accuracy: 0.8788\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 695us/step - loss: 0.1430 - accuracy: 0.9773\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 680us/step - loss: 0.1497 - accuracy: 0.9545\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 695us/step - loss: 0.1146 - accuracy: 0.9697\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 688us/step - loss: 0.1012 - accuracy: 0.9621\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 688us/step - loss: 0.1483 - accuracy: 0.9470\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 660us/step - loss: 0.0830 - accuracy: 0.9773\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 684us/step - loss: 0.0954 - accuracy: 0.9773\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 695us/step - loss: 0.0927 - accuracy: 0.9697\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 657us/step - loss: 0.0487 - accuracy: 0.9924\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 688us/step - loss: 0.0382 - accuracy: 0.9924\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 672us/step - loss: 0.0441 - accuracy: 0.9848\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 680us/step - loss: 0.0335 - accuracy: 0.9848\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 710us/step - loss: 0.0515 - accuracy: 0.9848\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 693us/step - loss: 0.0333 - accuracy: 0.9848\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 684us/step - loss: 0.0555 - accuracy: 0.9773\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 680us/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 659us/step - loss: 0.0554 - accuracy: 0.9848\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 680us/step - loss: 0.0294 - accuracy: 0.9924\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 647us/step - loss: 0.0184 - accuracy: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x19d000e7888>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 2ms/step\n",
      "\n",
      "Test accuracy =  0.8035714030265808\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:üòÑ prediction: he got a very nice raise\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: she got me a nice present\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: work is hard\tüòÑ\n",
      "Expected emoji:üòû prediction: This girl is messing with me\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: work is horrible\tüòÑ\n",
      "Expected emoji:üòÑ prediction: you brighten my day\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: she is a bully\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: My life is so boring\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: will you be my valentine\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: go away\t‚öæ\n",
      "Expected emoji:üòû prediction: yesterday we lost again\t‚öæ\n"
     ]
    }
   ],
   "source": [
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not feeling happy üòÑ\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['not feeling happy'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The system is not robust enough to correctly ascertain 'not happy' because the dataset size is very small"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
